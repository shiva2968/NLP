{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Summarization for ACER Technical Assignment \n",
    "#### Shiva Safaei\n",
    "The objective is to use a link on the internet and summarize its content (.txt or .pdf format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script is ready to use. The only part that we need to change is [here](#Data-Prep), where we can define the inputs: the link and the summary length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import heapq\n",
    "import requests \n",
    "import urllib\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import bs4 as bs\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A defined class for converting a PDF file to a text file;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfConverter:\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def convert_pdf_to_txt(self):\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        retstr = StringIO()\n",
    "        laparams = LAParams()\n",
    "        device = TextConverter(rsrcmgr, retstr, laparams = laparams)\n",
    "        fp = open(self.file_path, 'rb')\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        #pagenos = set()\n",
    "        for page in PDFPage.get_pages(fp, pagenos = set(), maxpages = 0, password = '',\n",
    "                                      caching = True, check_extractable = True):\n",
    "            interpreter.process_page(page)\n",
    "        fp.close()\n",
    "        device.close()\n",
    "        str = retstr.getvalue()\n",
    "        retstr.close()\n",
    "        return str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume `file_url` is our desired link which we want to find its summary. Also, here I chose 10 sentences (`top_sentences = 10`) for the length of the summary â€“this is quite optional!\n",
    "Below I showed how  to extract its text from different formatted texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Put the link and summary length below:  <a class=\"anchor\" id=\"Data-Prep\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = 'https://docs.financialaid.uic.edu/docs/PDF_upload_guide.pdf'\n",
    "top_sentences = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "if file_url.__contains__('pdf'):\n",
    "    r = requests.get(file_url, stream = True) \n",
    "    with open('dl_file.pdf', 'wb') as pdf:\n",
    "        for chunk in r.iter_content(chunk_size = 1024): \n",
    "            if chunk:\n",
    "                pdf.write(chunk)\n",
    "    pdfConverter = PdfConverter(file_path = 'dl_file.pdf')\n",
    "    article = pdfConverter.convert_pdf_to_txt()\n",
    "else:\n",
    "    url_text = urllib.request.urlopen(file_url)\n",
    "    txt = url_text.read()\n",
    "    parsed_txt = bs.BeautifulSoup(txt, 'lxml')\n",
    "    paragraphs = parsed_txt.find_all('p')\n",
    "    article = ''\n",
    "    for p in paragraphs:\n",
    "        article += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = re.sub(r'\\n', ' ', article)\n",
    "article = re.sub(r'\\[[0-9]*\\]', ' ', article)\n",
    "article = re.sub(r'\\s+', ' ', article)\n",
    "article = re.sub(r'\\s+', ' ', article)\n",
    "\n",
    "with open('article.txt', 'w') as text_file:\n",
    "    for i in  article:\n",
    "        text_file.write(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many methods for creating the summary of a text. Below is a simple way to extract the highlighted part of a text.\n",
    "This method is based on the higher frequency of appearing a word and evaluating the most important sentences.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_article = re.sub('[^a-zA-Z]', ' ', article)\n",
    "sentence_list = nltk.sent_tokenize(article)\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "word_frequencies = {}\n",
    "for word in nltk.word_tokenize(formatted_article):\n",
    "    if word not in stop_words:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "\n",
    "\n",
    "maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "\n",
    "sentence_scores = {}\n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "\n",
    "summary_sentences = heapq.nlargest(top_sentences, sentence_scores, key = sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "with open('summary_1.txt', 'w') as text_file:\n",
    "    for i in  summary:\n",
    "        text_file.write(i)                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is saved in `summary_1.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is using the similarity of the sentence vectors to summarize the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_smlrty(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1        \n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "def build_smlrty_mtrx(sentences, stop_words):\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences))) \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: \n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_smlrty(sentences[idx1], sentences[idx2], stop_words)\n",
    "            if np.isnan(similarity_matrix[idx1][idx2]):\n",
    "                similarity_matrix[idx1][idx2] = 0    \n",
    "    return similarity_matrix\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "summarize_text = []\n",
    "\n",
    "file = open('article.txt', \"r\")\n",
    "article = file.readlines()[0].split(\". \")\n",
    "sentences = []\n",
    "\n",
    "for sentence in article:\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "sentences.pop() \n",
    "sentence_smlrty_mtrx = build_smlrty_mtrx(sentences, stop_words)\n",
    "sentence_smlrty_grph = nx.from_numpy_array(sentence_smlrty_mtrx)\n",
    "scores = nx.pagerank(sentence_smlrty_grph, max_iter = 150)    \n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)      \n",
    "\n",
    "for i in range(top_sentences):\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "with open('summary_2.txt', 'w') as text_file:\n",
    "    for i in  summarize_text:\n",
    "        text_file.write(i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is saved in `summary_2.txt`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
